# -*- coding: utf-8 -*-
"""AML Project1

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Um447qTiYCMCAHzvvE3GL2hkqtlksTP9
"""

# -------------------------------------------------------------------
# STEP 1: SETUP & IMPORTS
# -------------------------------------------------------------------
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Preprocessing tools
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# Models
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier

# Evaluation Metrics
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score

# --- (Optional: Mount Google Drive if using Google Colab) ---
from google.colab import drive
drive.mount('/content/drive')

print("Libraries imported and Google Drive mounted.")

# -------------------------------------------------------------------
# STEP 2: LOAD & EXPLORE DATA
# -------------------------------------------------------------------

# !!! --- REPLACE THIS WITH THE ACTUAL PATH TO YOUR FILE --- !!!
file_path = '/content/drive/My Drive/AML Project/Heart Disease Dataset/heart.csv'

try:
    df = pd.read_csv(file_path)
except FileNotFoundError:
    print(f"Error: File not found at {file_path}")
    print("Please update the 'file_path' variable to the correct location in your Google Drive.")
    # Stop execution if file isn't found
    # In a real notebook, you'd stop the cell here.
    raise

# --- Initial Data Exploration ---
print("\n--- Data Head (First 5 Rows) ---")
print(df.head())

print("\n--- Data Info (Columns, Types, Nulls) ---")
df.info()

print("\n--- Data Description (Stats) ---")
print(df.describe())

# -------------------------------------------------------------------
# STEP 3: DATA PREPROCESSING
# -------------------------------------------------------------------

# !!! --- REPLACE THIS WITH YOUR ACTUAL TARGET COLUMN NAME --- !!!
# This is the column you are trying to predict (e.g., 'HeartFailure', 'DeathEvent', etc.)
TARGET_COLUMN = 'HeartDisease'

if TARGET_COLUMN not in df.columns:
    print(f"Error: Target column '{TARGET_COLUMN}' not found in the DataFrame.")
    print(f"Available columns are: {df.columns.tolist()}")
    # Stop execution
    raise

# --- Check for Class Imbalance ---
print(f"\n--- Target Variable Distribution ({TARGET_COLUMN}) ---")
print(df[TARGET_COLUMN].value_counts(normalize=True))
# This is important. If one class is 95% and the other is 5%, accuracy is a bad metric.

# --- 1. Define Features (X) and Target (y) ---
y = df[TARGET_COLUMN]
X = df.drop(TARGET_COLUMN, axis=1)

# --- 2. Handle Categorical Features (if any) ---
# Models need numbers, not text (e.g., 'Male', 'Female').
# pd.get_dummies() will automatically convert text columns into 0s and 1s.
X = pd.get_dummies(X, drop_first=True)
print(f"\nData shape after encoding categorical features: {X.shape}")

# --- 3. Split Data into Training and Testing Sets ---
# We save 20% of the data for a final test.
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)
# 'stratify=y' is crucial for imbalanced datasets. It ensures both train and test sets
# have the same percentage of positive/negative cases.

# --- 4. Scale Features ---
# This is CRITICAL for Logistic Regression and KNN.
# It makes sure features like 'age' (20-80) don't overpower features like 'ejection_fraction' (0-1).
# We fit the scaler ONLY on the training data, then use it to transform both.
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

print("Data preprocessing complete (Train/Test Split & Scaling).")

# -------------------------------------------------------------------
# STEP 4A: TUNE KNN & RANDOM FOREST FOR HIGH RECALL
# -------------------------------------------------------------------
from sklearn.model_selection import GridSearchCV

print("\n--- Hyperparameter Tuning for KNN (optimize Recall) ---")
knn = KNeighborsClassifier()

param_grid_knn = {
    'n_neighbors': [3, 5, 7, 9, 11, 15],
    'weights': ['uniform', 'distance'],
    'p': [1, 2]  # 1 = Manhattan, 2 = Euclidean
}

grid_knn = GridSearchCV(
    estimator=knn,
    param_grid=param_grid_knn,
    scoring='recall',   # true positive rate for class 1
    cv=5,
    n_jobs=-1
)

grid_knn.fit(X_train_scaled, y_train)
best_knn = grid_knn.best_estimator_

print("Best KNN params:", grid_knn.best_params_)
print("Best KNN CV Recall:", grid_knn.best_score_)


print("\n--- Hyperparameter Tuning for Random Forest (optimize Recall) ---")
rf = RandomForestClassifier(
    random_state=42,
    class_weight='balanced'  # make class 1 mistakes more expensive
)

param_grid_rf = {
    'n_estimators': [100, 200],
    'max_depth': [None, 10, 20],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4],
    'max_features': ['sqrt', 'log2']
}

grid_rf = GridSearchCV(
    estimator=rf,
    param_grid=param_grid_rf,
    scoring='recall',  # focus on TPR for HeartDisease=1
    cv=5,
    n_jobs=-1
)

grid_rf.fit(X_train_scaled, y_train)
best_rf = grid_rf.best_estimator_

print("Best RF params:", grid_rf.best_params_)
print("Best RF CV Recall:", grid_rf.best_score_)


# -------------------------------------------------------------------
# STEP 4B: DEFINE, TRAIN, & EVALUATE MODELS (USING OPTIMIZED ONES)
# -------------------------------------------------------------------

# 1. Use optimized KNN & RF in the models dict
models = {
    "Logistic Regression": LogisticRegression(random_state=42),
    "Optimized KNN": best_knn,
    "Optimized Random Forest": best_rf,
    "XGBoost": XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss')
}

# 2. Create a DataFrame to store results for comparison
results = pd.DataFrame(columns=['Model', 'Accuracy', 'Precision', 'Recall', 'F1-Score'])

# 3. Loop through each model, train it, and evaluate it
print("\n--- Model Training & Evaluation ---")
for name, model in models.items():

    # Train the model
    model.fit(X_train_scaled, y_train)

    # Make predictions on the test set (standard 0.5 threshold)
    y_pred = model.predict(X_test_scaled)

    # Calculate metrics
    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred)
    recall = recall_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred)

    # Store results
    results = pd.concat([results, pd.DataFrame({
        'Model': [name],
        'Accuracy': [accuracy],
        'Precision': [precision],
        'Recall': [recall],
        'F1-Score': [f1]
    })], ignore_index=True)

    # Print individual reports
    print(f"\n--- {name} ---")
    print(f"Accuracy: {accuracy:.4f}")
    print(f"Precision: {precision:.4f}")
    print(f"Recall (TPR): {recall:.4f}")
    print(f"F1-Score: {f1:.4f}")

    print("\nClassification Report:")
    print(classification_report(y_test, y_pred))

    print("Confusion Matrix:")
    cm = confusion_matrix(y_test, y_pred)
    print(cm)
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
    plt.title(f"{name} Confusion Matrix")
    plt.ylabel('Actual')
    plt.xlabel('Predicted')
    plt.show()

# -------------------------------------------------------------------
# STEP 4C: RANDOM FOREST - FIND A THRESHOLD WITH VERY LOW FALSE NEGATIVES
# -------------------------------------------------------------------
from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score

print("\n--- Optimized Random Forest: Threshold Sweep for High Recall ---")

# Probabilities for class 1
y_proba_rf = best_rf.predict_proba(X_test_scaled)[:, 1]

best_thr = None
best_fn = None
best_stats = None

# Try many thresholds between 0.05 and 0.95
for thr in np.linspace(0.05, 0.95, 19):
    y_pred_thr = (y_proba_rf >= thr).astype(int)
    cm = confusion_matrix(y_test, y_pred_thr)
    tn, fp, fn, tp = cm.ravel()

    # Store if this threshold gives fewer FNs
    if best_fn is None or fn < best_fn:
        best_fn = fn
        best_thr = thr
        best_stats = {
            "cm": cm,
            "precision": precision_score(y_test, y_pred_thr),
            "recall": recall_score(y_test, y_pred_thr),
            "f1": f1_score(y_test, y_pred_thr)
        }

print(f"\nBest threshold (fewest FNs) found: {best_thr:.2f}")
print("Confusion matrix at this threshold:")
print(best_stats["cm"])
print(f"Precision: {best_stats['precision']:.4f}")
print(f"Recall (TPR): {best_stats['recall']:.4f}")
print(f"F1-Score: {best_stats['f1']:.4f}")

tn, fp, fn, tp = best_stats["cm"].ravel()
print(f"True Positives: {tp}, False Negatives: {fn}")
print(f"True Negatives: {tn}, False Positives: {fp}")

# -------------------------------------------------------------------
# STEP 4D: HIGH-RECALL VERSIONS OF KNN & RANDOM FOREST
#          (MINIMIZE FALSE NEGATIVES BY CHANGING THRESHOLD)
# -------------------------------------------------------------------
def high_recall_confusion(model, X, y_true, label):
    """
    Sweep thresholds from 0.05 to 0.95, pick the one with the
    fewest false negatives, and show its confusion matrix.
    """
    # Probabilities for class 1 (HeartDisease = 1)
    y_proba = model.predict_proba(X)[:, 1]

    best_thr = None
    best_fn = None
    best_cm = None

    for thr in np.linspace(0.05, 0.95, 19):
        y_pred_thr = (y_proba >= thr).astype(int)
        cm = confusion_matrix(y_true, y_pred_thr)
        tn, fp, fn, tp = cm.ravel()

        if best_fn is None or fn < best_fn:
            best_fn = fn
            best_thr = thr
            best_cm = cm

    tn, fp, fn, tp = best_cm.ravel()

    print(f"\n{label}: threshold with FEWEST FNs = {best_thr:.2f}")
    print("Confusion matrix at this threshold:")
    print(best_cm)
    print(f"TP = {tp},  FN = {fn},  TN = {tn},  FP = {fp}")
    print("Recall (TPR):", recall_score(y_true, (y_proba >= best_thr).astype(int)))
    print("Precision:",   precision_score(y_true, (y_proba >= best_thr).astype(int)))
    print("F1-Score:",    f1_score(y_true, (y_proba >= best_thr).astype(int)))

    # Plot heatmap so you can visually compare
    plt.figure(figsize=(5,4))
    sns.heatmap(best_cm, annot=True, fmt='d', cmap='Blues')
    plt.title(f"{label} High-Recall Confusion Matrix (thr={best_thr:.2f})")
    plt.ylabel('Actual')
    plt.xlabel('Predicted')
    plt.show()


# Run high-recall search for BOTH models
high_recall_confusion(best_knn, X_test_scaled, y_test, "Optimized KNN")
high_recall_confusion(best_rf,  X_test_scaled, y_test, "Optimized Random Forest")

# -------------------------------------------------------------------
# STEP 5: COMPARE MODELS
# -------------------------------------------------------------------
print("\n--- Final Model Comparison ---")
results_sorted = results.sort_values(by='F1-Score', ascending=False)
print(results_sorted)

# Plotting the results for a clear visual comparison
plt.figure(figsize=(10, 6))
sns.barplot(data=results_sorted, x='Model', y='F1-Score')
plt.title('Model F1-Score Comparison')
plt.xticks(rotation=45)
plt.show()