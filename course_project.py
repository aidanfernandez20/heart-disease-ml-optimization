# -*- coding: utf-8 -*-
"""Course_Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19Ynyu3icDWc6iYemqDO9b6wLqRFx8dLg
"""

# -------------------------------------------------------------------
# STEP 1: SETUP & IMPORTS
# -------------------------------------------------------------------
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Preprocessing tools
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# Models
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier

# Evaluation Metrics
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score

# --- (Optional: Mount Google Drive if using Google Colab) ---
from google.colab import drive
drive.mount('/content/drive')

print("Libraries imported and Google Drive mounted.")

# -------------------------------------------------------------------
# STEP 2: LOAD & EXPLORE DATA
# -------------------------------------------------------------------

# !!! --- REPLACE THIS WITH THE ACTUAL PATH TO YOUR FILE --- !!!
file_path = '/content/drive/My Drive/Fall 2025/Machine Learning/Course Project/Dataset/heart.csv'

try:
    df = pd.read_csv(file_path)
except FileNotFoundError:
    print(f"Error: File not found at {file_path}")
    print("Please update the 'file_path' variable to the correct location in your Google Drive.")
    # Stop execution if file isn't found
    # In a real notebook, you'd stop the cell here.
    raise

# --- Initial Data Exploration ---
print("\n--- Data Head (First 5 Rows) ---")
print(df.head())

print("\n--- Data Info (Columns, Types, Nulls) ---")
df.info()

print("\n--- Data Description (Stats) ---")
print(df.describe())

# -------------------------------------------------------------------
# STEP 3: DATA PREPROCESSING
# -------------------------------------------------------------------

# !!! --- REPLACE THIS WITH YOUR ACTUAL TARGET COLUMN NAME --- !!!
# This is the column you are trying to predict (e.g., 'HeartFailure', 'DeathEvent', etc.)
TARGET_COLUMN = 'HeartDisease'

if TARGET_COLUMN not in df.columns:
    print(f"Error: Target column '{TARGET_COLUMN}' not found in the DataFrame.")
    print(f"Available columns are: {df.columns.tolist()}")
    # Stop execution
    raise

# --- Check for Class Imbalance ---
print(f"\n--- Target Variable Distribution ({TARGET_COLUMN}) ---")
print(df[TARGET_COLUMN].value_counts(normalize=True))
# This is important. If one class is 95% and the other is 5%, accuracy is a bad metric.

# --- 1. Define Features (X) and Target (y) ---
y = df[TARGET_COLUMN]
X = df.drop(TARGET_COLUMN, axis=1)

# --- 2. Handle Categorical Features (if any) ---
# Models need numbers, not text (e.g., 'Male', 'Female').
# pd.get_dummies() will automatically convert text columns into 0s and 1s.
X = pd.get_dummies(X, drop_first=True)
print(f"\nData shape after encoding categorical features: {X.shape}")

# --- 3. Split Data into Training and Testing Sets ---
# We save 20% of the data for a final test.
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)
# 'stratify=y' is crucial for imbalanced datasets. It ensures both train and test sets
# have the same percentage of positive/negative cases.

# --- 4. Scale Features ---
# This is CRITICAL for Logistic Regression and KNN.
# It makes sure features like 'age' (20-80) don't overpower features like 'ejection_fraction' (0-1).
# We fit the scaler ONLY on the training data, then use it to transform both.
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

print("Data preprocessing complete (Train/Test Split & Scaling).")

# -------------------------------------------------------------------
# STEP 4: DEFINE, TRAIN, & EVALUATE MODELS
# -------------------------------------------------------------------

# 1. Define all your models in a dictionary
models = {
    "Logistic Regression": LogisticRegression(random_state=42),
    "K-Nearest Neighbors (KNN)": KNeighborsClassifier(), # Default k=5
    "Random Forest": RandomForestClassifier(random_state=42),
    "XGBoost": XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss')
}

# 2. Create a DataFrame to store results for comparison
results = pd.DataFrame(columns=['Model', 'Accuracy', 'Precision', 'Recall', 'F1-Score'])

# 3. Loop through each model, train it, and evaluate it
print("\n--- Model Training & Evaluation ---")
for name, model in models.items():

    # Train the model
    # We use the scaled data for all models for a fair comparison.
    # While tree-based models (RF, XGB) don't *need* scaling, it doesn't hurt them.
    model.fit(X_train_scaled, y_train)

    # Make predictions on the test set
    y_pred = model.predict(X_test_scaled)

    # Calculate metrics
    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred) # Precision for the positive class (1)
    recall = recall_score(y_test, y_pred)     # Recall for the positive class (1)
    f1 = f1_score(y_test, y_pred)         # F1-Score for the positive class (1)

    # Store results
    results = pd.concat([results, pd.DataFrame({
        'Model': [name],
        'Accuracy': [accuracy],
        'Precision': [precision],
        'Recall': [recall],
        'F1-Score': [f1]
    })], ignore_index=True)

    # --- Print individual reports ---
    print(f"\n--- {name} ---")
    print(f"Accuracy: {accuracy:.4f}")
    print(f"Precision: {precision:.4f}")
    print(f"Recall: {recall:.4f}")
    print(f"F1-Score: {f1:.4f}")

    print("\nClassification Report:")
    print(classification_report(y_test, y_pred))

    print("Confusion Matrix:")
    cm = confusion_matrix(y_test, y_pred)
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
    plt.title(f"{name} Confusion Matrix")
    plt.ylabel('Actual')
    plt.xlabel('Predicted')
    plt.show()

# -------------------------------------------------------------------
# STEP 5: COMPARE MODELS
# -------------------------------------------------------------------
print("\n--- Final Model Comparison ---")
results_sorted = results.sort_values(by='F1-Score', ascending=False)
print(results_sorted)

# Plotting the results for a clear visual comparison
plt.figure(figsize=(10, 6))
sns.barplot(data=results_sorted, x='Model', y='F1-Score')
plt.title('Model F1-Score Comparison')
plt.xticks(rotation=45)
plt.show()